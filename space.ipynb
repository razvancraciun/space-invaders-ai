{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "space.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LE3atxGVstjj",
        "mFyS8v7c6Vuo",
        "r6d7LXAq9Bf3"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/razvancraciun/space-invaders-ai/blob/master/space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQaNHZ0qfMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = True\n",
        "LOAD = True\n",
        "LOAD_INDEX = 180\n",
        "\n",
        "\n",
        "# learning params\n",
        "EPISODES = 1000\n",
        "BATCH_SIZE = 32  # <! 256\n",
        "LEARNING_RATE = 0.001\n",
        "SAVE_PATH = '/content/drive/My Drive/save3/'\n",
        "SAVE_INTERVAL = 20\n",
        "\n",
        "# agent params\n",
        "GAMMA = 0.95\n",
        "EPSILON = 1\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DEC = 0.9995\n",
        "\n",
        "FRAME_STACK_SIZE = 3\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_hLZ5F6snHo",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57smLDC4qyC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFAgCvAgsh1X",
        "colab_type": "text"
      },
      "source": [
        "## Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF_zW2MgnJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size, state_shape, n_actions):\n",
        "        self.size = size\n",
        "        self.count = 0\n",
        "\n",
        "        self.from_stack = torch.zeros( (self.size, *state_shape), dtype=torch.float32)\n",
        "        self.to_stack = torch.zeros((self.size, *state_shape), dtype=torch.float32)\n",
        "        self.actions = torch.zeros(self.size, dtype=torch.long)\n",
        "        self.rewards = torch.zeros(self.size)\n",
        "        self.terminals = torch.zeros(self.size)\n",
        "\n",
        "    def store(self, from_stack, action, reward, to_stack, done):\n",
        "        index = self.count % self.size\n",
        "        \n",
        "        self.from_stack[index] = torch.Tensor(from_stack)\n",
        "        self.to_stack[index] = torch.Tensor(to_stack)\n",
        "        self.actions[index] = torch.Tensor([action])\n",
        "        self.rewards[index] = reward\n",
        "        self.terminals[index] = 1 - int(done)\n",
        "        self.count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        size = min(self.count, self.size)\n",
        "        batch = np.random.choice(size, batch_size)\n",
        "\n",
        "        from_states = self.from_stack[batch]\n",
        "        to_states = self.to_stack[batch]\n",
        "        actions = self.actions[batch]\n",
        "        rewards = self.rewards[batch]\n",
        "        terminals = self.terminals[batch]\n",
        "\n",
        "        return from_states, actions, rewards, to_states, terminals "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE3atxGVstjj",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOSRIyJ2qhpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_channels, output_shape, learning_rate):\n",
        "        super(Model, self).__init__()    \n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(4)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_shape)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), learning_rate)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.block1(x)\n",
        "        y = self.block2(y)\n",
        "        y = self.fc(y)\n",
        "        if torch.isnan(y).any():\n",
        "            print(y)\n",
        "        return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyS8v7c6Vuo",
        "colab_type": "text"
      },
      "source": [
        "## Frame handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMUHp-5y6adf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_frame(frame):\n",
        "    frame = preprocess(frame)\n",
        "    stack.pop(0)\n",
        "    stack.append(frame)\n",
        "\n",
        "\n",
        "def init_stack(frame):\n",
        "    frame = preprocess(frame)\n",
        "    for _ in range(FRAME_STACK_SIZE):\n",
        "        stack.append(frame)\n",
        "\n",
        "\n",
        "def preprocess(state):\n",
        "    state = np.moveaxis(state, 2, 0)\n",
        "    r,g,b = state[0], state[1], state[2]\n",
        "    state = 0.3 * r + 0.59 * g + 0.11 * b\n",
        "    state = state[20:-14, 15:-15]\n",
        "    return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6d7LXAq9Bf3",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H_gMzjm9AWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.epsilon_dec = EPSILON_DEC\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.gamma = GAMMA\n",
        "        self.action_space = range(n_actions)\n",
        "        print('Allocating model...')\n",
        "        self.model = Model(FRAME_STACK_SIZE, n_actions, LEARNING_RATE)\n",
        "        print('Done')\n",
        "        print('Allocating buffer...')\n",
        "        self.buffer = ReplayBuffer(BUFFER_SIZE, (FRAME_STACK_SIZE, 176, 130), n_actions)\n",
        "        print('Done')\n",
        "        self.device = default_device()\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def choose_action(self, stack):\n",
        "        stack = torch.Tensor(stack)\n",
        "        stack.unsqueeze_(0)\n",
        "        rand = np.random.rand()\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.model.forward(stack.clone().to(self.device))\n",
        "            _, action = torch.max(actions,1)\n",
        "            action = action.data.tolist()[0]\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        if self.buffer.count < self.batch_size:\n",
        "            return\n",
        "        from_states, actions, rewards, to_states, terminals = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        from_states = from_states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        to_states = to_states.to(self.device)\n",
        "        terminals = terminals.to(self.device)\n",
        "\n",
        "        q = self.model.forward(from_states).to(self.device)\n",
        "        q_next = self.model.forward(to_states).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "        q_target = q.clone()\n",
        "        batch_index = torch.arange(0, self.batch_size, dtype=torch.long)\n",
        "        max_next, _ = q_next.max(dim=1)\n",
        "        q_target[batch_index, actions] = (rewards + self.gamma * max_next * terminals).to(self.device)\n",
        "        \n",
        "\n",
        "        self.model.optimizer.zero_grad()\n",
        "        cost = self.model.loss(q, q_target)\n",
        "        cost.backward()\n",
        "        self.model.optimizer.step()\n",
        "\n",
        "        self.epsilon = self.epsilon * self.epsilon_dec \\\n",
        "            if self.epsilon > self.epsilon_min else self.epsilon_min\n",
        "\n",
        "    def save_model(self, index):\n",
        "        torch.save(self.model, SAVE_PATH + f'model{index}.pt')\n",
        "\n",
        "    def load_model(self, index):\n",
        "        self.model = torch.load(SAVE_PATH + f'model{index}.pt', map_location=default_device())\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "def default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI8_k5rd8-aR",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M6LPZebZRfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log(scores, index):\n",
        "\tf = open(SAVE_PATH + f'log{index}.txt', 'w')\n",
        "\tf.write(str(scores))\n",
        "\tf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZlEIQP2sTL_",
        "colab_type": "code",
        "outputId": "1df850bb-d2f7-42d5-dcb6-2790f323949d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "env = gym.make('SpaceInvaders-v0')\n",
        "agent = Agent(env.action_space.n)\n",
        "if LOAD:\n",
        "    print(f'Loading checkpoint with index {LOAD_INDEX}')\n",
        "    agent.load_model(LOAD_INDEX)\n",
        "    agent.epsilon = EPSILON_MIN\n",
        "start = LOAD_INDEX + 1 if LOAD else 0\n",
        "\n",
        "if TRAIN:\n",
        "    print('Training...')\n",
        "    scores = []\n",
        "    for episode in range(start, EPISODES+start):\n",
        "        stack = []\n",
        "        init_stack(env.reset())\n",
        "        done = False\n",
        "        score = 0\n",
        "        actions = []\n",
        "        while not done:\n",
        "            action = agent.choose_action(stack)\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done and info['ale.lives'] == 0:\n",
        "                reward = -100\n",
        "            old_stack = np.copy(stack)\n",
        "            add_frame(new_state)\n",
        "            agent.buffer.store(old_stack, action, reward, stack, done)\n",
        "            agent.train()\n",
        "        print(f'episode:{episode} score:{score}')\n",
        "        scores.append(score)\n",
        "        if episode % SAVE_INTERVAL == 0 and episode != 0:\n",
        "            agent.save_model(episode)\n",
        "            log(scores, episode)\n",
        "else:\n",
        "    print('Testing...')\n",
        "    agent.epsilon = 0\n",
        "    stack = []\n",
        "    init_stack(env.reset())\n",
        "    done = False\n",
        "    score = 0\n",
        "    img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    while not done:\n",
        "        img.set_data(env.render(mode='rgb_array'))\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        action = agent.choose_action(stack)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        old_stack = np.copy(stack)\n",
        "    print(f'Done! Score:{score}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating model...\n",
            "Done\n",
            "Allocating buffer...\n",
            "Done\n",
            "Loading checkpoint with index 180\n",
            "Training...\n",
            "episode:181 score:260.0\n",
            "episode:182 score:270.0\n",
            "episode:183 score:140.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h4DnWIauKdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# agent = Agent(6)\n",
        "# agent.load_model()\n",
        "# agent.epsilon = 0\n",
        "# stack = []\n",
        "# init_stack(env.reset())\n",
        "# done = False\n",
        "# score = 0\n",
        "# while not done:\n",
        "#     action = agent.choose_action(stack)\n",
        "#     new_state, reward, done, info = env.step(action)\n",
        "#     score += reward\n",
        "#     if done and info['ale.lives'] == 0:\n",
        "#         reward = -100\n",
        "#     old_stack = np.copy(stack)\n",
        "#     add_frame(new_state)\n",
        "# print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RiU-lP5ZpXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}