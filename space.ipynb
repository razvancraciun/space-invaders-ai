{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "space.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vFAgCvAgsh1X",
        "mFyS8v7c6Vuo",
        "r6d7LXAq9Bf3"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/razvancraciun/space-invaders-ai/blob/master/space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQaNHZ0qfMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# learning params\n",
        "EPISODES = 100\n",
        "BATCH_SIZE = 16  # <! 256\n",
        "LEARNING_RATE = 0.001\n",
        "SAVE_PATH = ''\n",
        "SAVE_INTERVAL = 20\n",
        "\n",
        "# agent params\n",
        "GAMMA = 0.95\n",
        "EPSILON = 1\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DEC = 0.9995\n",
        "\n",
        "FRAME_STACK_SIZE = 3\n",
        "BUFFER_SIZE = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_hLZ5F6snHo",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57smLDC4qyC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFAgCvAgsh1X",
        "colab_type": "text"
      },
      "source": [
        "## Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF_zW2MgnJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size, state_shape, n_actions):\n",
        "        self.size = size\n",
        "        self.count = 0\n",
        "\n",
        "        self.from_stack = torch.zeros( (self.size, *state_shape), dtype=torch.float32)\n",
        "        self.to_stack = torch.zeros((self.size, *state_shape), dtype=torch.float32)\n",
        "        self.actions = torch.zeros(self.size, dtype=torch.long)\n",
        "        self.rewards = torch.zeros(self.size)\n",
        "        self.terminals = torch.zeros(self.size)\n",
        "\n",
        "    def store(self, from_stack, action, reward, to_stack, done):\n",
        "        index = self.count % self.size\n",
        "        \n",
        "        self.from_stack[index] = torch.Tensor(from_stack)\n",
        "        self.to_stack[index] = torch.Tensor(to_stack)\n",
        "        self.actions[index] = torch.Tensor([action])\n",
        "        self.rewards[index] = reward\n",
        "        self.terminals[index] = 1 - int(done)\n",
        "        self.count += 1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        size = min(self.count, self.size)\n",
        "        batch = np.random.choice(size, batch_size)\n",
        "\n",
        "        from_states = self.from_stack[batch]\n",
        "        to_states = self.to_stack[batch]\n",
        "        actions = self.actions[batch]\n",
        "        rewards = self.rewards[batch]\n",
        "        terminals = self.terminals[batch]\n",
        "\n",
        "        return from_states, actions, rewards, to_states, terminals "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE3atxGVstjj",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOSRIyJ2qhpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_channels, output_shape, learning_rate):\n",
        "        super(Model, self).__init__()    \n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(4)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_shape)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), learning_rate)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.block1(x)\n",
        "        y = self.block2(y)\n",
        "        y = self.fc(y)\n",
        "        if torch.isnan(y).any():\n",
        "            print(y)\n",
        "        return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFyS8v7c6Vuo",
        "colab_type": "text"
      },
      "source": [
        "## Frame handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMUHp-5y6adf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_frame(frame):\n",
        "    frame = preprocess(frame)\n",
        "    stack.pop(0)\n",
        "    stack.append(frame)\n",
        "\n",
        "\n",
        "def init_stack(frame):\n",
        "    frame = preprocess(frame)\n",
        "    for _ in range(FRAME_STACK_SIZE):\n",
        "        stack.append(frame)\n",
        "\n",
        "\n",
        "def preprocess(state):\n",
        "    state = np.moveaxis(state, 2, 0)\n",
        "    r,g,b = state[0], state[1], state[2]\n",
        "    state = 0.3 * r + 0.59 * g + 0.11 * b\n",
        "    state = state[20:-14, 15:-15]\n",
        "    return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6d7LXAq9Bf3",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H_gMzjm9AWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.epsilon_dec = EPSILON_DEC\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.gamma = GAMMA\n",
        "        self.action_space = range(n_actions)\n",
        "        print('Allocating model...')\n",
        "        self.model = Model(FRAME_STACK_SIZE, n_actions, LEARNING_RATE)\n",
        "        print('Done')\n",
        "        print('Allocating buffer...')\n",
        "        self.buffer = ReplayBuffer(BUFFER_SIZE, (FRAME_STACK_SIZE, 176, 130), n_actions)\n",
        "        print('Done')\n",
        "        self.device = default_device()\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def choose_action(self, stack):\n",
        "        stack = torch.Tensor(stack)\n",
        "        stack.unsqueeze_(0)\n",
        "        rand = np.random.rand()\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            actions = self.model.forward(stack.clone().to(self.device))\n",
        "            _, action = torch.max(actions,1)\n",
        "            action = action.data.tolist()[0]\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        if self.buffer.count < self.batch_size:\n",
        "            return\n",
        "        from_states, actions, rewards, to_states, terminals = self.buffer.sample(self.batch_size)\n",
        "\n",
        "        from_states = from_states.to(self.device)\n",
        "        actions = actions.to(self.device)\n",
        "        rewards = rewards.to(self.device)\n",
        "        to_states = to_states.to(self.device)\n",
        "        terminals = terminals.to(self.device)\n",
        "\n",
        "        q = self.model.forward(from_states).to(self.device)\n",
        "        q_next = self.model.forward(to_states).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "        q_target = q.clone()\n",
        "        batch_index = torch.arange(0, self.batch_size, dtype=torch.long)\n",
        "        max_next, _ = q_next.max(dim=1)\n",
        "        q_target[batch_index, actions] = (rewards + self.gamma * max_next * terminals).to(self.device)\n",
        "        \n",
        "\n",
        "        self.model.optimizer.zero_grad()\n",
        "        cost = self.model.loss(q, q_target)\n",
        "        cost.backward()\n",
        "        self.model.optimizer.step()\n",
        "\n",
        "        self.epsilon = self.epsilon * self.epsilon_dec \\\n",
        "            if self.epsilon > self.epsilon_min else self.epsilon_min\n",
        "\n",
        "    def save_model():\n",
        "        torch.save(self.model, SAVE_PATH)\n",
        "\n",
        "    def load_model():\n",
        "        self.model = torch.load(SAVE_PATH)\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "def default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI8_k5rd8-aR",
        "colab_type": "text"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZlEIQP2sTL_",
        "colab_type": "code",
        "outputId": "d70bcf90-3608-421a-c46a-f3ba3e881ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = gym.make('SpaceInvaders-v0')\n",
        "agent = Agent(env.action_space.n)\n",
        "print('Training...')\n",
        "for episode in range(EPISODES):\n",
        "    stack = []\n",
        "    init_stack(env.reset())\n",
        "    done = False\n",
        "    score = 0\n",
        "    actions = []\n",
        "    while not done:\n",
        "        action = agent.choose_action(stack)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if done and info['ale.lives'] == 0:\n",
        "            reward = -100\n",
        "        old_stack = np.copy(stack)\n",
        "        add_frame(new_state)\n",
        "        agent.buffer.store(old_stack, action, reward, stack, done)\n",
        "        agent.train()\n",
        "    print(f'episode:{episode} score:{score}')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Allocating model...\n",
            "Done\n",
            "Allocating buffer...\n",
            "Done\n",
            "Training...\n",
            "episode:0 score:395.0\n",
            "episode:1 score:155.0\n",
            "episode:2 score:105.0\n",
            "episode:3 score:110.0\n",
            "episode:4 score:125.0\n",
            "episode:5 score:385.0\n",
            "episode:6 score:105.0\n",
            "episode:7 score:215.0\n",
            "episode:8 score:15.0\n",
            "episode:9 score:265.0\n",
            "episode:10 score:35.0\n",
            "episode:11 score:175.0\n",
            "episode:12 score:210.0\n",
            "episode:13 score:65.0\n",
            "episode:14 score:270.0\n",
            "episode:15 score:165.0\n",
            "episode:16 score:75.0\n",
            "episode:17 score:110.0\n",
            "episode:18 score:110.0\n",
            "episode:19 score:130.0\n",
            "episode:20 score:50.0\n",
            "episode:21 score:225.0\n",
            "episode:22 score:140.0\n",
            "episode:23 score:130.0\n",
            "episode:24 score:285.0\n",
            "episode:25 score:130.0\n",
            "episode:26 score:15.0\n",
            "episode:27 score:125.0\n",
            "episode:28 score:185.0\n",
            "episode:29 score:200.0\n",
            "episode:30 score:65.0\n",
            "episode:31 score:45.0\n",
            "episode:32 score:35.0\n",
            "episode:33 score:50.0\n",
            "episode:34 score:170.0\n",
            "episode:35 score:70.0\n",
            "episode:36 score:75.0\n",
            "episode:37 score:215.0\n",
            "episode:38 score:55.0\n",
            "episode:39 score:35.0\n",
            "episode:40 score:200.0\n",
            "episode:41 score:480.0\n",
            "episode:42 score:260.0\n",
            "episode:43 score:245.0\n",
            "episode:44 score:55.0\n",
            "episode:45 score:45.0\n",
            "episode:46 score:100.0\n",
            "episode:47 score:110.0\n",
            "episode:48 score:150.0\n",
            "episode:49 score:90.0\n",
            "episode:50 score:50.0\n",
            "episode:51 score:270.0\n",
            "episode:52 score:75.0\n",
            "episode:53 score:25.0\n",
            "episode:54 score:135.0\n",
            "episode:55 score:150.0\n",
            "episode:56 score:135.0\n",
            "episode:57 score:240.0\n",
            "episode:58 score:260.0\n",
            "episode:59 score:130.0\n",
            "episode:60 score:245.0\n",
            "episode:61 score:125.0\n",
            "episode:62 score:220.0\n",
            "episode:63 score:210.0\n",
            "episode:64 score:185.0\n",
            "episode:65 score:210.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}